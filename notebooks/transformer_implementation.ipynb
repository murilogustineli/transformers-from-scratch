{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c5fa3f",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "\n",
    "<img src=\"../images/transformer-architecture.png\" alt=\"transformer\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12909881",
   "metadata": {},
   "source": [
    "## 1. Tokenization + embeddings\n",
    "- Build char/word vocab\n",
    "- Map to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75684e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b33ee5a8",
   "metadata": {},
   "source": [
    "## 2. Positional encoding\n",
    "- Implementing sinusoidal, visualize the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac693cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd6c945",
   "metadata": {},
   "source": [
    "## 3. Scaled dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c1378",
   "metadata": {},
   "source": [
    "**Self-Attention** is a mechanism that captures dependencies and relationships within input sequences.\n",
    "\n",
    "**What it does** — for each token, self-attention builds a **contextualized vector** by mixing information from **all** tokens in the sequence, weighted by how relevant they are to that token.\n",
    "- It allows the model to identify and weight the importance of different parts of the input sequence by attending to itself.\n",
    "\n",
    "<img src=\"../images/scaled-dot-product-attn.png\" alt=\"scaled-dot-product-attn\" width=\"400\"/>\n",
    "\n",
    "#### Attention equation:\n",
    "$$\n",
    "\\operatorname{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})=\\underbrace{\\operatorname{softmax}\\left(\\frac{\\textbf{Q}\\textbf{K}^{\\top}}{\\sqrt{d_k}}+\\operatorname{mask}\\right)}_{\\text {attention weights } A} \\textbf{V}\n",
    "$$\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_{k}}, K \\in \\mathbb{R}^{n \\times d_{k}}, V \\in \\mathbb{R}^{n \\times d_{v}}$, with sequence length $ n$.\n",
    "- $A \\in \\mathbb{R}^{n \\times n}$ has **row-wise** softmax: each row sums to 1.\n",
    "\n",
    "**Why the scale $\\sqrt{d_{k}}$ ?**\n",
    "\n",
    "Dot products grow with dimension; dividing by $\\sqrt{ d_{k} }$ keeps logits in a range where softmax is well-behaved (prevents tiny gradients / saturation).\n",
    "\n",
    "#### Where $Q, K, V$ come from?\n",
    "$Q, K, V$ are **learned [linear projections](https://en.wikipedia.org/wiki/Projection_(linear_algebra))** (not literal copies) of the same input $H \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$\n",
    "Q=HW_{Q},\\ \\ K=HW_{K},\\ \\ V=HW_{V}\n",
    "$$\n",
    "\n",
    "This lets the model learn:\n",
    "- **Q (Query):** what to ask.\n",
    "- **K (Key):** how to index.\n",
    "- **V (Value):** what to retrieve.\n",
    "\n",
    "**Hash-table intuition (soft lookup)**\n",
    "- Compare each **query** to all **keys** via dot product –> *similarity scores*.\n",
    "- Softmax turns scores into a probability distribution –> *attention weights*.\n",
    "- Take a weighted sum of **values** using those weights –> the output for that token.\n",
    "\n",
    "#### Masks (what to zero out and why)\n",
    "- **Padding mask:** add $-\\infty$ to logits where positions are padding so their softmax weight becomes 0.\n",
    "- **Causal mask (decoder):** forbid looking ahead (future tokens).\n",
    "> The mask is added **before** softmax, it zeroes weights *after* softmax.\n",
    "\n",
    "#### What the $Q, K, V$ matrices mean\n",
    "- $QK^\\top$ –> an $ n \\times n$ **score matrix** where entry $(i, j)$ is how much token $i$ attends to token $j$.\n",
    "- After softmax, each row is a distribution over **which positions token $i$ should read from.**\n",
    "- Multiplying by $V$ aggregates information: token $i$ gets a weighted mix of other token’s value vectors.\n",
    "> Important: rows are **not** forced to be one-hot. They’re usually **soft** (spread across many tokens), especially in early layers.="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d40c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: torch.Size([3, 3])\n",
      "tensor([[0.7071, 0.7071, 0.0000],\n",
      "        [0.0000, 0.7071, 0.7071],\n",
      "        [0.7071, 1.4142, 0.7071]])\n",
      "\n",
      "attn_weights shape: torch.Size([3, 3])\n",
      "tensor([[0.4011, 0.4011, 0.1978],\n",
      "        [0.1978, 0.4011, 0.4011],\n",
      "        [0.2483, 0.5035, 0.2483]])\n",
      "\n",
      "attn_output shape: torch.Size([3, 2])\n",
      "tensor([[0.5989, 1.0000],\n",
      "        [0.5989, 1.2033],\n",
      "        [0.4965, 1.2552]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3 tokens, head_dim = 2 tensors\n",
    "Q = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "K = torch.tensor([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "V = torch.tensor([[1.0, 0.0], [0.0, 2.0], [1.0, 1.0]])\n",
    "L, d_h = Q.shape  # L=3, d_h=2\n",
    "\n",
    "# attention scores\n",
    "scores = (Q @ K.T) / (d_h**0.5)\n",
    "print(f\"scores shape: {scores.shape}\")\n",
    "print(scores)\n",
    "\n",
    "# attention weights: softmax over keys dimension\n",
    "attn_weights = F.softmax(scores, dim=-1)  # [L, L]\n",
    "print(f\"\\nattn_weights shape: {attn_weights.shape}\")\n",
    "print(attn_weights)\n",
    "\n",
    "# output: [L, d_h]\n",
    "attn_output = attn_weights @ V\n",
    "print(f\"\\nattn_output shape: {attn_output.shape}\")\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7ec01",
   "metadata": {},
   "source": [
    "## 4. Multi-head attention\n",
    "\n",
    "<img src=\"../images/multi-head-attn.png\" alt=\"multi-head-attn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6040b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f172be",
   "metadata": {},
   "source": [
    "## 5. Feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a402291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f73161",
   "metadata": {},
   "source": [
    "## 6. Residual + LayerNorm\n",
    "\n",
    "In the [Attention is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper, the original diagram shows the residual connection and LayerNorm being applied ***after*** each sublayer (Multi-Head Attention and Feed-Forward Network). \n",
    "- **Multi-Head Attention -> Add & Norm**\n",
    "- **Feed-Forward Network -> Add & Norm**\n",
    "\n",
    "This design is often called **Post-LN (post-layer normalization)**.\n",
    "\n",
    "Later work found training to be unstable with the original Post-LN architecture, especially for deeper models. To fix this, the LayerNorm was moved to the input of each sublayer:\n",
    "- **LayerNorm -> Multi-Head Attention -> Add**\n",
    "- **LayerNorm -> Feed-Forward Network -> Add**\n",
    "\n",
    "This is called **Pre-LN (pre-layer normalization)**. It stabilizes gradients and makes optimization easier for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bd6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5c0e98",
   "metadata": {},
   "source": [
    "## 7. Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddb7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79eeb08f",
   "metadata": {},
   "source": [
    "## 8. Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cd0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b026ddbe",
   "metadata": {},
   "source": [
    "## 9. Tiny Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25bee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7debf4f9",
   "metadata": {},
   "source": [
    "## 10. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991fce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb34ef0a",
   "metadata": {},
   "source": [
    "## 11. Sampling / generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4941c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
