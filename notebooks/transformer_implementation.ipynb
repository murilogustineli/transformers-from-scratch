{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c5fa3f",
   "metadata": {},
   "source": [
    "# Transformers from Scratch\n",
    "\n",
    "<img src=\"../images/transformer-architecture.png\" alt=\"transformer\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12909881",
   "metadata": {},
   "source": [
    "## 1. Tokenization + embeddings\n",
    "- Build char/word vocab\n",
    "- Map to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75684e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b33ee5a8",
   "metadata": {},
   "source": [
    "## 2. Positional encoding\n",
    "- Implementing sinusoidal, visualize the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac693cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd6c945",
   "metadata": {},
   "source": [
    "## 3. Scaled dot-product attention\n",
    "\n",
    "<img src=\"../images/scaled-dot-product-attn.png\" alt=\"scaled-dot-product-attn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d40c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3 tokens, head_dim = 2 tensors\n",
    "Q = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "K = torch.tensor([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "V = torch.tensor([[1.0, 0.0], [0.0, 2.0], [1.0, 1.0]])\n",
    "L, d_h = Q.shape  # L=3, d_h=2\n",
    "\n",
    "# attention scores\n",
    "scores = (Q @ K.T) / (d_h**0.5)\n",
    "print(f\"scores shape: {scores.shape}\")\n",
    "print(scores)\n",
    "\n",
    "# attention weights: softmax over keys dimension\n",
    "attn_weights = F.softmax(scores, dim=-1)  # [L, L]\n",
    "print(f\"\\nattn_weights shape: {attn_weights.shape}\")\n",
    "print(attn_weights)\n",
    "\n",
    "# output: [L, d_h]\n",
    "attn_output = attn_weights @ V\n",
    "print(f\"\\nattn_output shape: {attn_output.shape}\")\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7ec01",
   "metadata": {},
   "source": [
    "## 4. Multi-head attention\n",
    "\n",
    "<img src=\"../images/multi-head-attn.png\" alt=\"multi-head-attn\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f172be",
   "metadata": {},
   "source": [
    "## 5. Feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a402291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f73161",
   "metadata": {},
   "source": [
    "## 6. Residual + LayerNorm\n",
    "\n",
    "In the [Attention is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper, the original diagram shows the residual connection and LayerNorm being applied ***after*** each sublayer (Multi-Head Attention and Feed-Forward Network). \n",
    "- **Multi-Head Attention -> Add & Norm**\n",
    "- **Feed-Forward Network -> Add & Norm**\n",
    "\n",
    "This design is often called **Post-LN (post-layer normalization)**.\n",
    "\n",
    "Later work found training to be unstable with the original Post-LN architecture, especially for deeper models. To fix this, the LayerNorm was moved to the input of each sublayer:\n",
    "- **LayerNorm -> Multi-Head Attention -> Add**\n",
    "- **LayerNorm -> Feed-Forward Network -> Add**\n",
    "\n",
    "This is called **Pre-LN (pre-layer normalization)**. It stabilizes gradients and makes optimization easier for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bd6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5c0e98",
   "metadata": {},
   "source": [
    "## 7. Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddb7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79eeb08f",
   "metadata": {},
   "source": [
    "## 8. Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cd0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b026ddbe",
   "metadata": {},
   "source": [
    "## 9. Tiny Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25bee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7debf4f9",
   "metadata": {},
   "source": [
    "## 10. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991fce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb34ef0a",
   "metadata": {},
   "source": [
    "## 11. Sampling / generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4941c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
