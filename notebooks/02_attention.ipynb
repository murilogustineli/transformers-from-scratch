{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b46507e",
   "metadata": {},
   "source": [
    "# Scaled Dot-Product Attention\n",
    "\n",
    "**Self-Attention** is a mechanism that captures dependencies and relationships within input sequences.\n",
    "\n",
    "**What it does** — for each token, self-attention builds a **contextualized vector** by mixing information from **all** tokens in the sequence, weighted by how relevant they are to that token.\n",
    "- It allows the model to identify and weight the importance of different parts of the input sequence by attending to itself.\n",
    "\n",
    "<img src=\"../images/scaled-dot-product-attn.png\" alt=\"scaled-dot-product-attn\" width=\"400\"/>\n",
    "\n",
    "#### Attention equation:\n",
    "$$\n",
    "\\operatorname{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V})=\\underbrace{\\operatorname{softmax}\\left(\\frac{\\textbf{Q}\\textbf{K}^{\\top}}{\\sqrt{d_k}}+\\operatorname{mask}\\right)}_{\\text {attention weights } A} \\textbf{V}\n",
    "$$\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_{k}}, K \\in \\mathbb{R}^{n \\times d_{k}}, V \\in \\mathbb{R}^{n \\times d_{v}}$, with sequence length $ n$.\n",
    "- $A \\in \\mathbb{R}^{n \\times n}$ has **row-wise** softmax: each row sums to 1.\n",
    "\n",
    "**Why the scale $\\sqrt{d_{k}}$ ?**\n",
    "\n",
    "Dot products grow with dimension; dividing by $\\sqrt{ d_{k} }$ keeps logits in a range where softmax is well-behaved (prevents tiny gradients / saturation).\n",
    "\n",
    "#### Where $Q, K, V$ come from?\n",
    "$Q, K, V$ are **learned [linear projections](https://en.wikipedia.org/wiki/Projection_(linear_algebra))** (not literal copies) of the same input $H \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$\n",
    "Q=HW_{Q},\\ \\ K=HW_{K},\\ \\ V=HW_{V}\n",
    "$$\n",
    "\n",
    "This lets the model learn:\n",
    "- **Q (Query):** what to ask.\n",
    "- **K (Key):** how to index.\n",
    "- **V (Value):** what to retrieve.\n",
    "\n",
    "**Hash-table intuition (soft lookup)**\n",
    "- Compare each **query** to all **keys** via dot product –> *similarity scores*.\n",
    "- Softmax turns scores into a probability distribution –> *attention weights*.\n",
    "- Take a weighted sum of **values** using those weights –> the output for that token.\n",
    "\n",
    "#### Masks (what to zero out and why)\n",
    "- **Padding mask:** add $-\\infty$ to logits where positions are padding so their softmax weight becomes 0.\n",
    "- **Causal mask (decoder):** forbid looking ahead (future tokens).\n",
    "> The mask is added **before** softmax, it zeroes weights *after* softmax.\n",
    "\n",
    "#### What the $Q, K, V$ matrices mean\n",
    "- $QK^\\top$ –> an $ n \\times n$ **score matrix** where entry $(i, j)$ is how much token $i$ attends to token $j$.\n",
    "- After softmax, each row is a distribution over **which positions token $i$ should read from.**\n",
    "- Multiplying by $V$ aggregates information: token $i$ gets a weighted mix of other token’s value vectors.\n",
    "> Important: rows are **not** forced to be one-hot. They’re usually **soft** (spread across many tokens), especially in early layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4d1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: torch.Size([3, 3])\n",
      "tensor([[0.7071, 0.7071, 0.0000],\n",
      "        [0.0000, 0.7071, 0.7071],\n",
      "        [0.7071, 1.4142, 0.7071]])\n",
      "\n",
      "attn_weights shape: torch.Size([3, 3])\n",
      "tensor([[0.4011, 0.4011, 0.1978],\n",
      "        [0.1978, 0.4011, 0.4011],\n",
      "        [0.2483, 0.5035, 0.2483]])\n",
      "\n",
      "attn_output shape: torch.Size([3, 2])\n",
      "tensor([[0.5989, 1.0000],\n",
      "        [0.5989, 1.2033],\n",
      "        [0.4965, 1.2552]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3 tokens, head_dim = 2 tensors\n",
    "Q = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "K = torch.tensor([[1.0, 0.0], [1.0, 1.0], [0.0, 1.0]])\n",
    "V = torch.tensor([[1.0, 0.0], [0.0, 2.0], [1.0, 1.0]])\n",
    "L, d_h = Q.shape  # L=3, d_h=2\n",
    "\n",
    "# attention scores\n",
    "scores = (Q @ K.T) / (d_h**0.5)\n",
    "print(f\"scores shape: {scores.shape}\")\n",
    "print(scores)\n",
    "\n",
    "# attention weights: softmax over keys dimension\n",
    "attn_weights = F.softmax(scores, dim=-1)  # [L, L]\n",
    "print(f\"\\nattn_weights shape: {attn_weights.shape}\")\n",
    "print(attn_weights)\n",
    "\n",
    "# output: [L, d_h]\n",
    "attn_output = attn_weights @ V\n",
    "print(f\"\\nattn_output shape: {attn_output.shape}\")\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7ab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
